{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ç§‘ç³»æ•´ç†"
      ],
      "metadata": {
        "id": "U8PGU7l7Wd9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##å…ˆæ•´ç†å¤§å­¸çš„ç§‘ç³»èˆ‡å°æ‡‰"
      ],
      "metadata": {
        "id": "1z4cIHFgWg0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdnnHt1ZTbDi"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# é¸æ“‡ä¸Šå‚³ Excel æª”æ¡ˆ\n",
        "uploaded = files.upload()\n",
        "\n",
        "# å–å¾—æª”æ¡ˆåç¨±\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# è®€å– Excel\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_data = []\n",
        "for index, row in df.iterrows():\n",
        "    if pd.notna(row[\"discrib\"]):  # ç¢ºä¿ç¬¬äº”æ¬„æœ‰å€¼\n",
        "        split_values = row[\"discrib\"].split(\"ã€\")  # ä¾æ“šé “è™Ÿåˆ‡å‰²\n",
        "        for value in split_values:\n",
        "            expanded_data.append([row[\"mainid\"], row[\"midid\"], row[\"lastid\"], value, row[\"eduid\"]])  # é‡æ–°çµ„åˆæˆæ–°è¡Œ\n",
        "    else:\n",
        "        expanded_data.append([row[\"mainid\"], row[\"midid\"], row[\"lastid\"], row[\"lastname\"], row[\"eduid\"]])  # æ²’æœ‰ç¬¬äº”æ¬„çš„ä¿ç•™åŸå§‹\n",
        "\n",
        "# è½‰æ›ç‚ºæ–°çš„ DataFrame\n",
        "expanded_df = pd.DataFrame(expanded_data, columns=[\"mainid\", \"midid\", \"lastid\t\", \"lastname\", \"eduid\"])"
      ],
      "metadata": {
        "id": "zS_BWx3wT3Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df.head(10)"
      ],
      "metadata": {
        "id": "p1y9WbIyUyus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å‡è¨­ df æ˜¯ä½ çš„ DataFrame\n",
        "output_file = \"output.xlsx\"  # è¼¸å‡ºçš„æª”æ¡ˆåç¨±\n",
        "\n",
        "# å°‡ DataFrame å„²å­˜ç‚º Excel\n",
        "expanded_df.to_excel(output_file, index=False)\n",
        "\n",
        "# è®“ Colab ä¸‹è¼‰ Excel æª”æ¡ˆ\n",
        "from google.colab import files\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "OybaBP02Uzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ckip èˆ‡ jeiba"
      ],
      "metadata": {
        "id": "oAlTfqCZWcHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "! pip install -U ckip-transformers\n",
        "from ckip_transformers import __version__\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "from google.colab import files\n",
        "import jieba\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GYVPyzTisKyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ–‡æœ¬é è™•ç†ï¼Œé€™è£¡å¯ä»¥ä½¿ç”¨ jieba é€²è¡Œä¸­æ–‡åˆ†è©\n",
        "def jieba_text(text):\n",
        "    # ä½¿ç”¨ jieba åˆ†è©\n",
        "    words = jieba.cut(text)\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "-S1eXG9bsW6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show version\n",
        "print(__version__)\n",
        "\n",
        "# Initialize drivers\n",
        "print(\"Initializing drivers ... WS\")\n",
        "ws_driver = CkipWordSegmenter(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... POS\")\n",
        "pos_driver = CkipPosTagger(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... NER\")\n",
        "ner_driver = CkipNerChunker(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... all done\")\n",
        "print()\n",
        "\n",
        "#model æœ‰å…¶å®ƒçš„å¯ä»¥é¸ï¼Œå¦‚ \"bert-base\"\n",
        "#device=0 æ˜¯ä½¿ç”¨ GPUï¼Œ device=-1 æ˜¯ä½¿ç”¨ CPUï¼Œä¸æŒ‡å®šä¹Ÿå¯ä»¥ã€‚\n",
        "\n",
        "\n",
        "def clean(sentence_ws, sentence_pos):\n",
        "  short_with_pos = []\n",
        "  short_sentence = []\n",
        "  stop_pos = set(['Nep', 'Nh', 'Nb',]) # é€™ 3 ç¨®è©æ€§ä¸ä¿ç•™\n",
        "  for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "    # åªç•™åè©å’Œå‹•è©\n",
        "    is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\") or word_pos.startswith(\"A\")\n",
        "    # å»æ‰åè©è£¡çš„æŸäº›è©æ€§\n",
        "    is_not_stop_pos = word_pos not in stop_pos\n",
        "    # åªå‰©ä¸€å€‹å­—çš„è©ä¹Ÿä¸ç•™\n",
        "    is_not_one_charactor = not (len(word_ws) == 1)\n",
        "    # çµ„æˆä¸²åˆ—\n",
        "    if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
        "      short_with_pos.append(f\"{word_ws}({word_pos})\")\n",
        "      short_sentence.append(f\"{word_ws}\")\n",
        "  return (\" \".join(short_sentence), \" \".join(short_with_pos))\n"
      ],
      "metadata": {
        "id": "u3MvCcmHsY5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #ä¸Šå‚³è³‡æ–™\n",
        "# Get the filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]\n",
        "data = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "Vg41fblPsHD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ç¢ºèªæ˜¯å¦æœ‰äº‚ç¢¼\n",
        "with open(filename, \"rb\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            line.decode(\"utf-8\")\n",
        "        except UnicodeDecodeError as e:\n",
        "            print(f\"éŒ¯èª¤ç™¼ç”Ÿåœ¨ç¬¬ {i+1} è¡Œï¼ŒéŒ¯èª¤è¨Šæ¯: {e}\")\n",
        "            break\n",
        "print(data.dtypes)\n",
        "print(data.head(10))"
      ],
      "metadata": {
        "id": "qbNr2gIasd8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=['mainid'], inplace=True)\n",
        "data['mainid'] = data['mainid'].astype(int)"
      ],
      "metadata": {
        "id": "9zNdfbXLQBmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å„²å­˜åˆ†è©çµæœèˆ‡æ–°çš„å¥å­\n",
        "data[\"result\"] = \"\"\n",
        "data[\"n_lastname\"] = \"\"\n",
        "\n",
        "# é€²è¡Œæ¯å€‹éƒ¨ä»½çš„åˆ†æ\n",
        "for index, row in data.iterrows():\n",
        "    description = row[\"lastname\"]\n",
        "    # åˆ†è©\n",
        "    ws_result = ws_driver([description])\n",
        "    # è©æ€§æ¨™è¨»\n",
        "    pos_result = pos_driver(ws_result)\n",
        "\n",
        "    # æ¸…æ´—æ•¸æ“š\n",
        "    cleaned_sentence, cleaned_with_pos = clean(ws_result[0], pos_result[0])\n",
        "\n",
        "    # ä¿å­˜ç»“æœ\n",
        "    data.at[index, \"result\"] = cleaned_with_pos\n",
        "    data.at[index, \"n_lastname\"] = cleaned_sentence"
      ],
      "metadata": {
        "id": "Pn3RXLPNsi-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['jeiba_lastname'] = data['lastname'].apply(jieba_text)\n",
        "data['n_lastname'] = data['n_lastname'].replace('', np.nan)\n",
        "data['n_lastname'] = data['n_lastname'].fillna(data['jeiba_lastname'])\n",
        "\n",
        "\n",
        "#ä¿®æ­£jeiba æœƒæ˜¯NAçš„ç‹€æ³\n",
        "print(data['jeiba_lastname'].isnull().sum())\n",
        "# ç¯©é¸å‡º 'jeiba_lastname' æ¬„ä½ç‚º NaN çš„æ•´ç­†è³‡æ–™\n",
        "missing_data = data[data['jeiba_lastname'].isnull()]\n",
        "\n",
        "# é¡¯ç¤ºé€™äº›éºå¤±çš„è³‡æ–™\n",
        "#print(missing_data)  # é¡¯ç¤ºæ‰€æœ‰éºå¤±çš„è³‡æ–™\n",
        "data['jeiba_lastname'] = data['jeiba_lastname'].fillna(data['lastname'])\n",
        "'''\n",
        "nan_indices = data[data['n_lastname'].isna()].index\n",
        "print(nan_indices)\n",
        "nan_rows_details = data.loc[[425, 2172]]\n",
        "print(nan_rows_details)\n",
        "\n",
        "ä¸åˆ†ç³» è·ŸåŸºç£æ•™å­¸ç³»æœƒè¢«nan\n",
        "\n",
        "'''\n",
        "print(data['n_lastname'].isnull().sum())\n",
        "data['n_lastname'] = data['n_lastname'].str.lstrip() #ç”±æ–¼é€£çµjeiba æœƒå°‡æœ€å‰é¢ç©ºæ ¼ä¹Ÿä¿ç•™\n",
        "print(data['n_lastname'].isnull().sum())"
      ],
      "metadata": {
        "id": "bPF0KlsHvXHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['n_lastname'].isnull().sum())\n",
        "print(data.iloc[676])"
      ],
      "metadata": {
        "id": "Qfd4iwD6s_Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_indices = data[data['n_lastname'].isna()].index\n",
        "print(nan_indices)\n",
        "nan_rows_details = data.loc[[425, 2172]]\n",
        "nan_rows_details"
      ],
      "metadata": {
        "id": "I0kBOB3-2yeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"2022ç§‘ç³»åˆ†é¡_å¥•å˜‰_å·²ç¶“åˆ†è©.csv\"\n",
        "data.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "c-46RxgZ20oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##åˆ†é¡æ¸¬è©¦"
      ],
      "metadata": {
        "id": "6bRd98heWct0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from google.colab import files\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "hXGnYSzp00TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### çµæœä¸ç†æƒ³ï¼Œæ‰€ä»¥å°‡data2 ä¸­éƒ¨ä»½è³‡æ–™èˆ‡data çµåˆ"
      ],
      "metadata": {
        "id": "suoGz6imW1Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ä¸Šå‚³è³‡æ–™ å…©ä»½è³‡æ–™\n",
        "uploaded = files.upload() #ä¸Šå‚³è³‡æ–™\n",
        "file1 = list(uploaded.keys())[0]\n",
        "df1 = pd.read_csv(file1)\n",
        "\n",
        "\n",
        "uploaded = files.upload() #ä¸Šå‚³è³‡æ–™\n",
        "file2 = list(uploaded.keys())[0]\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "#åˆä½µ\n",
        "data = pd.concat([df1, df2], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "V2K0LSbfdFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#æº–ç¢ºç‡ä¸é«˜ï¼Œæ‰€ä»¥æ¡ç”¨smote å¾Œå†é€²è¡Œ\n",
        "\n",
        "# æª¢æŸ¥é¡åˆ¥åˆ†éƒ¨\n",
        "data_df = pd.DataFrame({'lastname': data['n_lastname'], 'label': data['mainid']})\n",
        "label_counts = data_df['label'].value_counts()\n",
        "\n",
        "# æŸ¥çœ‹æ˜¯å¦æœ‰å°‘æ–¼5ç­†çš„æ¨£æœ¬\n",
        "rare_labels = label_counts[label_counts < 6].index\n",
        "print(f\"ç¨€æœ‰é¡åˆ¥: {rare_labels}\")\n"
      ],
      "metadata": {
        "id": "nrVfTRineQDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è™•ç†ä¸å‡è¡¡æ•¸æ“š (ä½¿ç”¨ SMOTE)\n",
        "X = data_df['lastname'].tolist()\n",
        "y = data_df['label'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# å°‡æ–‡æœ¬æ•¸æ“šè½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µ\n",
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=1, max_df=0.9, ngram_range=(1,2))  # å–å‰ 5000 å€‹é«˜é »è©\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "# å–®ç´”åˆ†å‰²æ•¸æ“š\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    X_vectorized, y_encoded, test_size=0.2, random_state=69, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# ä½¿ç”¨ SMOTE å¹³è¡¡é¡åˆ¥åˆ†ä½ˆ\n",
        "smote = SMOTE(sampling_strategy='not majority', random_state=42, k_neighbors=3) #è¤‡è£½not minority\n",
        "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y_encoded)\n",
        "\n",
        "# åˆ†å‰²æ•¸æ“šé›†_smote\n",
        "train_texts_smote, val_texts_smote, train_labels_smote, val_labels_smote = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=69, stratify=y_resampled\n",
        ")\n"
      ],
      "metadata": {
        "id": "h79ZrWyNeTmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### éš¨æ©Ÿæ£®æ—"
      ],
      "metadata": {
        "id": "3p4tIEe20euj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨“ç·´æ¨¡å‹ï¼šé€™è£¡ç”¨è¨“ç·´ç‰¹å¾µå’Œè¨“ç·´æ¨™ç±¤\n",
        "rdf_model = RandomForestClassifier(n_estimators=1000, random_state=42,max_depth= None,min_samples_split=2, min_samples_leaf=1,class_weight='balanced')\n",
        "rdf_model.fit(train_texts, train_labels)\n",
        "\n",
        "# é æ¸¬å’Œè©•ä¼°ï¼šç”¨é©—è­‰ç‰¹å¾µé€²è¡Œé æ¸¬ï¼Œä¸¦å°‡é æ¸¬çµæœèˆ‡é©—è­‰æ¨™ç±¤æ¯”è¼ƒ\n",
        "y_pred = rdf_model.predict(val_texts)\n",
        "print(classification_report(val_labels, y_pred))"
      ],
      "metadata": {
        "id": "yTiZfH8BnKNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨“ç·´æ¨¡å‹ï¼šé€™è£¡ç”¨è¨“ç·´ç‰¹å¾µå’Œè¨“ç·´æ¨™ç±¤ _smote\n",
        "rdf_model_smote = RandomForestClassifier(n_estimators=1000, random_state=42,max_depth= None,min_samples_split=2, min_samples_leaf=1,class_weight='balanced')\n",
        "rdf_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "# é æ¸¬å’Œè©•ä¼°ï¼šç”¨é©—è­‰ç‰¹å¾µé€²è¡Œé æ¸¬ï¼Œä¸¦å°‡é æ¸¬çµæœèˆ‡é©—è­‰æ¨™ç±¤æ¯”è¼ƒ\n",
        "y_pred_smote = rdf_model_smote.predict(val_texts_smote)\n",
        "print(classification_report(val_labels_smote, y_pred_smote))"
      ],
      "metadata": {
        "id": "rQ_YieXjeWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###æœ´ç´ è²"
      ],
      "metadata": {
        "id": "8FMgfYzK3J4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "cvhucDqb5iTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote\n",
        "nb_model_smote = MultinomialNB(alpha=0.1)\n",
        "nb_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_model_nb_smote = nb_model_smote.predict(val_texts_smote)\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels_smote, y_pred_model_nb_smote))"
      ],
      "metadata": {
        "id": "bqehfndL3QeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#åŸå§‹\n",
        "nb_model = MultinomialNB(alpha=0.1)\n",
        "nb_model.fit(train_texts, train_labels)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_model_nb = nb_model.predict(val_texts)\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels, y_pred_model_nb))"
      ],
      "metadata": {
        "id": "vGzGqZVW5Fbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM"
      ],
      "metadata": {
        "id": "LeYtRWJwy8ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "XNfnz7Gj5gAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote\n",
        "svm_model_smote = SVC(kernel='linear', C=1.3, class_weight='balanced', probability=True, random_state=69)\n",
        "svm_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_svm_smote = svm_model_smote.predict(val_texts_smote)\n",
        "\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels_smote, y_pred_svm_smote))"
      ],
      "metadata": {
        "id": "-gnVJmupyaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#måŸå§‹\n",
        "svm_model = SVC(kernel='linear', C=1.3, class_weight='balanced', probability=True, random_state=69)\n",
        "svm_model.fit(train_texts, train_labels)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_svm = svm_model.predict(val_texts)\n",
        "\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels, y_pred_svm))"
      ],
      "metadata": {
        "id": "ZTHD7SRoyokj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LightGBM\n"
      ],
      "metadata": {
        "id": "axDLP4RNP2p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "VA45wvGmQEWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Zero feature ratio:\", np.mean(train_texts_smote.toarray() == 0))\n"
      ],
      "metadata": {
        "id": "srdujCp9QojT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_model_smote = lgb.LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.005,\n",
        "    max_depth=-1,          # å…è¨±ç„¡é™æ·±åº¦\n",
        "    num_leaves=100,         # å¢åŠ è‘‰å­æ•¸\n",
        "    min_child_samples=2,   # æ¸›å°‘æœ€å°ç¯€é»æ¨£æœ¬æ•¸\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# è¨“ç·´ LightGBM æ¨¡å‹\n",
        "lgbm_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_lgbm_smote = lgbm_model_smote.predict(val_texts_smote)\n",
        "\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels_smote, y_pred_lgbm_smote))"
      ],
      "metadata": {
        "id": "YN-YeyQXP63z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_model = lgb.LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=-1,          # å…è¨±ç„¡é™æ·±åº¦\n",
        "    num_leaves=50,         # å¢åŠ è‘‰å­æ•¸\n",
        "    min_child_samples=2,   # æ¸›å°‘æœ€å°ç¯€é»æ¨£æœ¬æ•¸\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# è¨“ç·´ LightGBM æ¨¡å‹\n",
        "lgbm_model.fit(train_texts, train_labels)\n",
        "\n",
        "# é æ¸¬\n",
        "y_pred_lgbm= lgbm_model.predict(val_texts)\n",
        "\n",
        "# è©•ä¼°\n",
        "print(classification_report(val_labels, y_pred_lgbm))"
      ],
      "metadata": {
        "id": "lwGOsDM4R7ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å°ç­”æ¡ˆ\n"
      ],
      "metadata": {
        "id": "xr7ck-Ekmrzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #ä¸Šå‚³è³‡æ–™\n",
        "# Get the filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]\n",
        "data2 = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "DqoQ1RBrmuc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data2.dtypes)\n",
        "data2['n_lastname'] = data2['n_lastname'].fillna(data2['jeiba_lastname'])\n",
        "data2['n_lastname'] = data2['n_lastname'].fillna(data2['lastname'])"
      ],
      "metadata": {
        "id": "E7g1fPb6EDHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data2['n_lastname'].isnull().sum())"
      ],
      "metadata": {
        "id": "oQWv5XBZMWFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(new_data, model):\n",
        "    # å–å¾—é æ¸¬çµæœï¼Œæ³¨æ„é€™è£¡å‡è¨­æ¨¡å‹æ¥å— ln_lastname æ¬„ä½ä½œç‚ºè¼¸å…¥\n",
        "    # å°‡æ–‡æœ¬æ•¸æ“šè½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µ\n",
        "    X_new = vectorizer.transform(new_data['n_lastname'])\n",
        "\n",
        "    predicted_ids = model.predict(X_new)\n",
        "\n",
        "    # ä½¿ç”¨ç›¸åŒçš„ label_encoder å°‡ç·¨ç¢¼æ•¸å­—è½‰å›åŸå§‹æ¨™ç±¤\n",
        "    predicted_labels = label_encoder.inverse_transform(predicted_ids)\n",
        "\n",
        "    # å°‡é æ¸¬çµæœåŠ å…¥ DataFrame ä¸­\n",
        "    new_data['predicted_label'] = predicted_labels\n",
        "\n",
        "    # æ ¹æ“šé æ¸¬çµæœèˆ‡åŸå§‹ mainid é€²è¡Œæ¯”è¼ƒï¼Œç”¢ç”Ÿ code æ¬„ä½ (1 è¡¨ç¤ºé æ¸¬æ­£ç¢ºï¼Œ0 è¡¨ç¤ºéŒ¯èª¤)\n",
        "    new_data['code'] = (new_data['predicted_label'] == new_data['mainid']).astype(int)\n",
        "\n",
        "    # è¨ˆç®—æº–ç¢ºç‡\n",
        "    accuracy = new_data['code'].mean()\n",
        "    return new_data, accuracy"
      ],
      "metadata": {
        "id": "2asFTPRoZJsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###å°‡å„æ¨¡å‹çµæœå„²å­˜"
      ],
      "metadata": {
        "id": "0-NXv2uXu0z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#å–®ä¸€æ¨¡å‹æ¸¬è©¦\n",
        "data3, accu = predict_and_evaluate(data2, svm_model_smote)\n",
        "print(f\"æº–ç¢ºç‡: {accu:.4f}\")"
      ],
      "metadata": {
        "id": "5hHi4qSf0Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"rdf\": rdf_model,\n",
        "    \"rdf_smote\": rdf_model_smote,\n",
        "    \"nb\": nb_model,\n",
        "    \"nb_smote\": nb_model_smote,\n",
        "    \"svm\": svm_model,\n",
        "    \"svm_smote\": svm_model_smote,\n",
        "    \"lgbm\": lgbm_model,\n",
        "    \"lgbm_smote\": lgbm_model_smote\n",
        "}\n",
        "\n",
        "# ç”¨å­—å…¸å­˜å„²ä¸åŒæ¨¡å‹çš„é æ¸¬çµæœ\n",
        "results_df = {}  # å­˜æ”¾ DataFrame\n",
        "accuracies = {}  # å­˜æ”¾æº–ç¢ºç‡\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    results_df[model_name], accuracies[model_name] = predict_and_evaluate(data2.copy(), model)\n",
        "    print(f\"{model_name}: Accuracy = {accuracies[model_name]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "njVi19EFl400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#å°‡æ¨¡å‹çµæœåˆ†åˆ¥å„²å­˜\n",
        "rdf_df = results_df[\"rdf\"]\n",
        "rdf_smote_df = results_df[\"rdf_smote\"]\n",
        "nb_df = results_df[\"nb\"]\n",
        "nb_smote_df = results_df[\"nb_smote\"]\n",
        "svm_df = results_df[\"svm\"]\n",
        "svm_smote_df = results_df[\"svm_smote\"]\n",
        "lgbm_df = results_df[\"lgbm\"]\n",
        "lgbm_smote_df = results_df[\"lgbm_smote\"]"
      ],
      "metadata": {
        "id": "Yw3J_qiNpmKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, df in results_df.items():\n",
        "    filename = f\"{model_name}_predictions.csv\"  # ç”Ÿæˆæª”å\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    files.download(filename)  # ä¸‹è¼‰æª”æ¡ˆï¼ˆé©ç”¨æ–¼ Google Colabï¼‰\n",
        "    print(f\"âœ… {filename} å·²å„²å­˜ï¼\")"
      ],
      "metadata": {
        "id": "W78ckWleruTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###æª¢æŸ¥æ¨¡å‹çµæœ\n"
      ],
      "metadata": {
        "id": "thEBTcMsu8ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdf_smote_df.head())"
      ],
      "metadata": {
        "id": "ktIUvAzx6Jfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é¸æ“‡è¦åˆä½µçš„ DataFrame\n",
        "dfs_to_merge = [rdf_smote_df, nb_smote_df, svm_smote_df, lgbm_smote_df]\n",
        "\n",
        "# å…ˆç‚ºæ¯å€‹ DataFrame å…§çš„ predicted_label å’Œ code åŠ ä¸Šå°ˆå±¬å¾Œç¶´ï¼Œé¿å…è¡çª\n",
        "suffixes = [\"_rdf\", \"_nb\", \"_svm\", \"_lgbm\"]\n",
        "for i, df in enumerate(dfs_to_merge):\n",
        "    df.rename(columns={\"predicted_label\": f\"predicted_label{suffixes[i]}\",\n",
        "                       \"code\": f\"code{suffixes[i]}\"}, inplace=True)\n",
        "\n",
        "# ä»¥ç¬¬ä¸€å€‹ DataFrame ä½œç‚ºåŸºæº–ï¼Œä¾æ¬¡åˆä½µ\n",
        "merged_smote_df = dfs_to_merge[0]\n",
        "for df in dfs_to_merge[1:]:\n",
        "    merged_smote_df = merged_smote_df.merge(\n",
        "        df,\n",
        "        on=[\"mainid\", \"lastname\", \"result\", \"n_lastname\", \"jeiba_lastname\", \"n\", \"N\", \"k\"],\n",
        "        how=\"outer\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "EDwwVt9ku7LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdf_smote_df"
      ],
      "metadata": {
        "id": "ClHBCPJV8GR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##é›†æˆåˆ†é¡"
      ],
      "metadata": {
        "id": "Zn9J4edi6_lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# å»ºç«‹ä¸€å€‹ DataFrame å­˜æ”¾æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ\n",
        "ensemble_results = pd.DataFrame()\n",
        "\n",
        "# å…ˆåŠ å…¥çœŸå¯¦æ¨™ç±¤\n",
        "ensemble_results[\"true_label\"] = data2[\"mainid\"]\n",
        "ensemble_results[\"n_lastname\"] = data2[\"n_lastname\"]  # åŠ å…¥ n_lastname\n",
        "# åŠ å…¥å„æ¨¡å‹çš„é æ¸¬æ¨™ç±¤\n",
        "ensemble_results[\"rdf_pred\"] = rdf_smote_df['predicted_label_rdf']\n",
        "ensemble_results[\"nb_pred\"] = nb_smote_df['predicted_label_nb']\n",
        "ensemble_results[\"svm_pred\"] = svm_smote_df['predicted_label_svm']\n",
        "ensemble_results[\"lgbm_pred\"] = lgbm_smote_df['predicted_label_lgbm']\n",
        "\n",
        "# æª¢æŸ¥çµæœ\n",
        "#print(ensemble_results.head())"
      ],
      "metadata": {
        "id": "4BLk-r137L_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# è¨­å®šæ¬Šé‡ï¼ˆNB æœ€é«˜ï¼ŒLGBM æ¬¡é«˜ï¼ŒRF æœ€ä½ï¼‰\n",
        "model_weights = {\n",
        "    \"nb_pred\": 3,   # NB æ¬Šé‡æœ€é«˜\n",
        "    \"lgbm_pred\": 2, # LGBM æ¬Šé‡ä¸­ç­‰\n",
        "    \"rdf_pred\": 1    # éš¨æ©Ÿæ£®æ—æ¬Šé‡æœ€ä½\n",
        "}\n",
        "\n",
        "def weighted_voting(row):\n",
        "    \"\"\"ç•¶ SVM é æ¸¬éŒ¯èª¤æ™‚ï¼Œä½¿ç”¨åŠ æ¬ŠæŠ•ç¥¨\"\"\"\n",
        "    svm_pred = row[\"svm_pred\"]\n",
        "    true_label = row[\"true_label\"]\n",
        "\n",
        "    # å¦‚æœ SVM é æ¸¬æ­£ç¢ºï¼Œç›´æ¥ä½¿ç”¨ SVM çµæœ\n",
        "    if svm_pred == true_label:\n",
        "        return svm_pred\n",
        "\n",
        "    # ä½¿ç”¨ NBã€LGBMã€RF é€²è¡ŒåŠ æ¬ŠæŠ•ç¥¨\n",
        "    vote_scores = defaultdict(int)\n",
        "\n",
        "    for model, weight in model_weights.items():\n",
        "        predicted_class = row[model]  # å–å¾—è©²æ¨¡å‹çš„é æ¸¬çµæœ\n",
        "        vote_scores[predicted_class] += weight  # ç´¯åŠ è©²é¡åˆ¥çš„æ¬Šé‡\n",
        "\n",
        "    # å–å¾—æ¬Šé‡æœ€é«˜çš„é¡åˆ¥\n",
        "    final_prediction = max(vote_scores, key=vote_scores.get)\n",
        "    return final_prediction\n",
        "\n",
        "# å¥—ç”¨åŠ æ¬ŠæŠ•ç¥¨ä¿®æ­£\n",
        "ensemble_results[\"final_pred\"] = ensemble_results.apply(weighted_voting, axis=1)\n",
        "\n",
        "# è¨ˆç®—æœ€çµ‚æº–ç¢ºç‡\n",
        "final_accuracy = (ensemble_results[\"final_pred\"] == ensemble_results[\"true_label\"]).mean()\n",
        "print(f\"ğŸ¯ æœ€çµ‚åŠ æ¬ŠæŠ•ç¥¨ä¿®æ­£å¾Œçš„æº–ç¢ºç‡ï¼š{final_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Rt8MksfO7B1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#è¼¸å‡ºçµæœ\n",
        "output_filename = \"é æ¸¬çµæœæ¯”å°.csv\"\n",
        "ensemble_results.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "QCpl1jR4oK5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###å„²å­˜æ¨¡å‹"
      ],
      "metadata": {
        "id": "bJlzzE_vtvXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"rdf\": rdf_model,\n",
        "    \"rdf_smote\": rdf_model_smote,\n",
        "    \"nb\": nb_model,\n",
        "    \"nb_smote\": nb_model_smote,\n",
        "    \"svm\": svm_model,\n",
        "    \"svm_smote\": svm_model_smote,\n",
        "    \"lgbm\": lgbm_model,\n",
        "    \"lgbm_smote\": lgbm_model_smote\n",
        "}"
      ],
      "metadata": {
        "id": "SMho_Icj6u2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "# å„²å­˜æ‰€æœ‰æ¨¡å‹\n",
        "joblib.dump(svm_model_smote, 'svm.joblib')\n",
        "joblib.dump(nb_model_smote, 'naive_bayes.joblib')\n",
        "joblib.dump(rdf_model_smote, 'random_forest.joblib')\n",
        "joblib.dump(lgbm_model_smote, 'lightgbm.joblib')\n",
        "\n",
        "# å„²å­˜å‘é‡åŒ–å™¨ & æ¨™ç±¤ç·¨ç¢¼å™¨\n",
        "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
        "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
        "\n",
        "\n",
        "# ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹\n",
        "for file_name in [\"svm.joblib\", \"naive_bayes.joblib\", \"random_forest.joblib\", \"lightgbm.joblib\",\n",
        "                  \"vectorizer.joblib\", \"label_encoder.joblib\"]:\n",
        "    files.download(file_name)\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰æ¨¡å‹å·²å„²å­˜ä¸¦ä¸‹è¼‰ï¼\")\n"
      ],
      "metadata": {
        "id": "gq15jguWlIuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"random_forest.joblib\")"
      ],
      "metadata": {
        "id": "Mb3y5YmJCdxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä¸Šå‚³æ¨¡å‹é€²è¡Œä½¿ç”¨"
      ],
      "metadata": {
        "id": "lwj-6zCivJqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# è¼‰å…¥æ¨¡å‹\n",
        "model = joblib.load('model.joblib')\n",
        "\n",
        "# è¼‰å…¥ TF-IDF vectorizer\n",
        "vectorizer = joblib.load('vectorizer.joblib')\n",
        "\n",
        "# è¼‰å…¥ LabelEncoder\n",
        "label_encoder = joblib.load('label_encoder.joblib')"
      ],
      "metadata": {
        "id": "O4AiCTBzvfnq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
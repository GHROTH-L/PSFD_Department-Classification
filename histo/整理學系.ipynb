{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 科系整理"
      ],
      "metadata": {
        "id": "U8PGU7l7Wd9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##先整理大學的科系與對應"
      ],
      "metadata": {
        "id": "1z4cIHFgWg0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdnnHt1ZTbDi"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# 選擇上傳 Excel 檔案\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 取得檔案名稱\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# 讀取 Excel\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# 顯示前幾筆資料\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_data = []\n",
        "for index, row in df.iterrows():\n",
        "    if pd.notna(row[\"discrib\"]):  # 確保第五欄有值\n",
        "        split_values = row[\"discrib\"].split(\"、\")  # 依據頓號切割\n",
        "        for value in split_values:\n",
        "            expanded_data.append([row[\"mainid\"], row[\"midid\"], row[\"lastid\"], value, row[\"eduid\"]])  # 重新組合成新行\n",
        "    else:\n",
        "        expanded_data.append([row[\"mainid\"], row[\"midid\"], row[\"lastid\"], row[\"lastname\"], row[\"eduid\"]])  # 沒有第五欄的保留原始\n",
        "\n",
        "# 轉換為新的 DataFrame\n",
        "expanded_df = pd.DataFrame(expanded_data, columns=[\"mainid\", \"midid\", \"lastid\t\", \"lastname\", \"eduid\"])"
      ],
      "metadata": {
        "id": "zS_BWx3wT3Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df.head(10)"
      ],
      "metadata": {
        "id": "p1y9WbIyUyus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設 df 是你的 DataFrame\n",
        "output_file = \"output.xlsx\"  # 輸出的檔案名稱\n",
        "\n",
        "# 將 DataFrame 儲存為 Excel\n",
        "expanded_df.to_excel(output_file, index=False)\n",
        "\n",
        "# 讓 Colab 下載 Excel 檔案\n",
        "from google.colab import files\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "OybaBP02Uzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ckip 與 jeiba"
      ],
      "metadata": {
        "id": "oAlTfqCZWcHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "! pip install -U ckip-transformers\n",
        "from ckip_transformers import __version__\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "from google.colab import files\n",
        "import jieba\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GYVPyzTisKyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文本預處理，這裡可以使用 jieba 進行中文分詞\n",
        "def jieba_text(text):\n",
        "    # 使用 jieba 分詞\n",
        "    words = jieba.cut(text)\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "-S1eXG9bsW6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show version\n",
        "print(__version__)\n",
        "\n",
        "# Initialize drivers\n",
        "print(\"Initializing drivers ... WS\")\n",
        "ws_driver = CkipWordSegmenter(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... POS\")\n",
        "pos_driver = CkipPosTagger(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... NER\")\n",
        "ner_driver = CkipNerChunker(model=\"albert-base\", device=-1)\n",
        "print(\"Initializing drivers ... all done\")\n",
        "print()\n",
        "\n",
        "#model 有其它的可以選，如 \"bert-base\"\n",
        "#device=0 是使用 GPU， device=-1 是使用 CPU，不指定也可以。\n",
        "\n",
        "\n",
        "def clean(sentence_ws, sentence_pos):\n",
        "  short_with_pos = []\n",
        "  short_sentence = []\n",
        "  stop_pos = set(['Nep', 'Nh', 'Nb',]) # 這 3 種詞性不保留\n",
        "  for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "    # 只留名詞和動詞\n",
        "    is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\") or word_pos.startswith(\"A\")\n",
        "    # 去掉名詞裡的某些詞性\n",
        "    is_not_stop_pos = word_pos not in stop_pos\n",
        "    # 只剩一個字的詞也不留\n",
        "    is_not_one_charactor = not (len(word_ws) == 1)\n",
        "    # 組成串列\n",
        "    if is_N_or_V and is_not_stop_pos and is_not_one_charactor:\n",
        "      short_with_pos.append(f\"{word_ws}({word_pos})\")\n",
        "      short_sentence.append(f\"{word_ws}\")\n",
        "  return (\" \".join(short_sentence), \" \".join(short_with_pos))\n"
      ],
      "metadata": {
        "id": "u3MvCcmHsY5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #上傳資料\n",
        "# Get the filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]\n",
        "data = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "Vg41fblPsHD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#確認是否有亂碼\n",
        "with open(filename, \"rb\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            line.decode(\"utf-8\")\n",
        "        except UnicodeDecodeError as e:\n",
        "            print(f\"錯誤發生在第 {i+1} 行，錯誤訊息: {e}\")\n",
        "            break\n",
        "print(data.dtypes)\n",
        "print(data.head(10))"
      ],
      "metadata": {
        "id": "qbNr2gIasd8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset=['mainid'], inplace=True)\n",
        "data['mainid'] = data['mainid'].astype(int)"
      ],
      "metadata": {
        "id": "9zNdfbXLQBmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oOLTPavkNoof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 儲存分詞結果與新的句子\n",
        "data[\"result\"] = \"\"\n",
        "data[\"n_lastname\"] = \"\"\n",
        "\n",
        "# 進行每個部份的分析\n",
        "for index, row in data.iterrows():\n",
        "    description = row[\"lastname\"]\n",
        "    # 分詞\n",
        "    ws_result = ws_driver([description])\n",
        "    # 詞性標註\n",
        "    pos_result = pos_driver(ws_result)\n",
        "\n",
        "    # 清洗數據\n",
        "    cleaned_sentence, cleaned_with_pos = clean(ws_result[0], pos_result[0])\n",
        "\n",
        "    # 保存结果\n",
        "    data.at[index, \"result\"] = cleaned_with_pos\n",
        "    data.at[index, \"n_lastname\"] = cleaned_sentence"
      ],
      "metadata": {
        "id": "Pn3RXLPNsi-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['jeiba_lastname'] = data['lastname'].apply(jieba_text)\n",
        "data['n_lastname'] = data['n_lastname'].replace('', np.nan)\n",
        "data['n_lastname'] = data['n_lastname'].fillna(data['jeiba_lastname']).str.lstrip()\n",
        "\n",
        "\n",
        "#修正jeiba 會是NA的狀況\n",
        "print(data['jeiba_lastname'].isnull().sum())\n",
        "# 篩選出 'jeiba_lastname' 欄位為 NaN 的整筆資料\n",
        "missing_data = data[data['jeiba_lastname'].isnull()]\n",
        "\n",
        "# 顯示這些遺失的資料\n",
        "#print(missing_data)  # 顯示所有遺失的資料\n",
        "data['jeiba_lastname'] = data['jeiba_lastname'].fillna(data['lastname'])\n",
        "'''\n",
        "nan_indices = data[data['n_lastname'].isna()].index\n",
        "print(nan_indices)\n",
        "nan_rows_details = data.loc[[425, 2172]]\n",
        "print(nan_rows_details)\n",
        "\n",
        "不分系 跟基督教學系會被nan\n",
        "\n",
        "'''\n",
        "print(data['n_lastname'].isnull().sum())\n",
        "data['n_lastname'] = data['n_lastname'].str.lstrip() #由於連結jeiba 會將最前面空格也保留\n",
        "print(data['n_lastname'].isnull().sum())"
      ],
      "metadata": {
        "id": "bPF0KlsHvXHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['n_lastname'].isnull().sum())\n",
        "print(data.iloc[676])"
      ],
      "metadata": {
        "id": "Qfd4iwD6s_Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_indices = data[data['n_lastname'].isna()].index\n",
        "print(nan_indices)\n",
        "nan_rows_details = data.loc[[425, 2172]]\n",
        "nan_rows_details"
      ],
      "metadata": {
        "id": "I0kBOB3-2yeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"2022科系分類_奕嘉_已經分詞.csv\"\n",
        "data.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "c-46RxgZ20oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##分類測試"
      ],
      "metadata": {
        "id": "6bRd98heWct0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from google.colab import files\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "hXGnYSzp00TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 結果不理想，所以將data2 中部份資料與data 結合"
      ],
      "metadata": {
        "id": "suoGz6imW1Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#上傳資料 兩份資料\n",
        "uploaded = files.upload() #上傳資料\n",
        "file1 = list(uploaded.keys())[0]\n",
        "df1 = pd.read_csv(file1)\n",
        "\n",
        "\n",
        "uploaded = files.upload() #上傳資料\n",
        "file2 = list(uploaded.keys())[0]\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "#合併\n",
        "data = pd.concat([df1, df2], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "V2K0LSbfdFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#準確率不高，所以採用smote 後再進行\n",
        "\n",
        "# 檢查類別分部\n",
        "data_df = pd.DataFrame({'lastname': data['n_lastname'], 'label': data['mainid']})\n",
        "label_counts = data_df['label'].value_counts()\n",
        "\n",
        "# 查看是否有少於5筆的樣本\n",
        "rare_labels = label_counts[label_counts < 6].index\n",
        "print(f\"稀有類別: {rare_labels}\")\n"
      ],
      "metadata": {
        "id": "nrVfTRineQDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 處理不均衡數據 (使用 SMOTE)\n",
        "X = data_df['lastname'].tolist()\n",
        "y = data_df['label'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# 將文本數據轉換為數值特徵\n",
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=1, max_df=0.9, ngram_range=(1,2))  # 取前 5000 個高頻詞\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "# 單純分割數據\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    X_vectorized, y_encoded, test_size=0.2, random_state=69, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# 使用 SMOTE 平衡類別分佈\n",
        "smote = SMOTE(sampling_strategy='not majority', random_state=42, k_neighbors=3) #複製not minority\n",
        "X_resampled, y_resampled = smote.fit_resample(X_vectorized, y_encoded)\n",
        "\n",
        "# 分割數據集_smote\n",
        "train_texts_smote, val_texts_smote, train_labels_smote, val_labels_smote = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=69, stratify=y_resampled\n",
        ")\n"
      ],
      "metadata": {
        "id": "h79ZrWyNeTmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 隨機森林"
      ],
      "metadata": {
        "id": "3p4tIEe20euj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練模型：這裡用訓練特徵和訓練標籤\n",
        "rdf_model = RandomForestClassifier(n_estimators=1000, random_state=42,max_depth= None,min_samples_split=2, min_samples_leaf=1,class_weight='balanced')\n",
        "rdf_model.fit(train_texts, train_labels)\n",
        "\n",
        "# 預測和評估：用驗證特徵進行預測，並將預測結果與驗證標籤比較\n",
        "y_pred = rdf_model.predict(val_texts)\n",
        "print(classification_report(val_labels, y_pred))"
      ],
      "metadata": {
        "id": "yTiZfH8BnKNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練模型：這裡用訓練特徵和訓練標籤 _smote\n",
        "rdf_model_smote = RandomForestClassifier(n_estimators=1000, random_state=42,max_depth= None,min_samples_split=2, min_samples_leaf=1,class_weight='balanced')\n",
        "rdf_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "# 預測和評估：用驗證特徵進行預測，並將預測結果與驗證標籤比較\n",
        "y_pred_smote = rdf_model_smote.predict(val_texts_smote)\n",
        "print(classification_report(val_labels_smote, y_pred_smote))"
      ],
      "metadata": {
        "id": "rQ_YieXjeWDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###朴素貝"
      ],
      "metadata": {
        "id": "8FMgfYzK3J4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "cvhucDqb5iTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote\n",
        "nb_model_smote = MultinomialNB(alpha=0.1)\n",
        "nb_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# 預測\n",
        "y_pred_model_nb_smote = nb_model_smote.predict(val_texts_smote)\n",
        "# 評估\n",
        "print(classification_report(val_labels_smote, y_pred_model_nb_smote))"
      ],
      "metadata": {
        "id": "bqehfndL3QeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#原始\n",
        "nb_model = MultinomialNB(alpha=0.1)\n",
        "nb_model.fit(train_texts, train_labels)\n",
        "\n",
        "# 預測\n",
        "y_pred_model_nb = nb_model.predict(val_texts)\n",
        "# 評估\n",
        "print(classification_report(val_labels, y_pred_model_nb))"
      ],
      "metadata": {
        "id": "vGzGqZVW5Fbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM"
      ],
      "metadata": {
        "id": "LeYtRWJwy8ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "XNfnz7Gj5gAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote\n",
        "svm_model_smote = SVC(kernel='linear', C=1.3, class_weight='balanced', probability=True, random_state=69)\n",
        "svm_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# 預測\n",
        "y_pred_svm_smote = svm_model_smote.predict(val_texts_smote)\n",
        "\n",
        "# 評估\n",
        "print(classification_report(val_labels_smote, y_pred_svm_smote))"
      ],
      "metadata": {
        "id": "-gnVJmupyaYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#m原始\n",
        "svm_model = SVC(kernel='linear', C=1.3, class_weight='balanced', probability=True, random_state=69)\n",
        "svm_model.fit(train_texts, train_labels)\n",
        "\n",
        "# 預測\n",
        "y_pred_svm = svm_model.predict(val_texts)\n",
        "\n",
        "# 評估\n",
        "print(classification_report(val_labels, y_pred_svm))"
      ],
      "metadata": {
        "id": "ZTHD7SRoyokj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LightGBM\n"
      ],
      "metadata": {
        "id": "axDLP4RNP2p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "VA45wvGmQEWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Zero feature ratio:\", np.mean(train_texts_smote.toarray() == 0))\n"
      ],
      "metadata": {
        "id": "srdujCp9QojT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_model_smote = lgb.LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.005,\n",
        "    max_depth=-1,          # 允許無限深度\n",
        "    num_leaves=100,         # 增加葉子數\n",
        "    min_child_samples=2,   # 減少最小節點樣本數\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 訓練 LightGBM 模型\n",
        "lgbm_model_smote.fit(train_texts_smote, train_labels_smote)\n",
        "\n",
        "# 預測\n",
        "y_pred_lgbm_smote = lgbm_model_smote.predict(val_texts_smote)\n",
        "\n",
        "# 評估\n",
        "print(classification_report(val_labels_smote, y_pred_lgbm_smote))"
      ],
      "metadata": {
        "id": "YN-YeyQXP63z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_model = lgb.LGBMClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=-1,          # 允許無限深度\n",
        "    num_leaves=50,         # 增加葉子數\n",
        "    min_child_samples=2,   # 減少最小節點樣本數\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 訓練 LightGBM 模型\n",
        "lgbm_model.fit(train_texts, train_labels)\n",
        "\n",
        "# 預測\n",
        "y_pred_lgbm= lgbm_model.predict(val_texts)\n",
        "\n",
        "# 評估\n",
        "print(classification_report(val_labels, y_pred_lgbm))"
      ],
      "metadata": {
        "id": "lwGOsDM4R7ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 對答案\n"
      ],
      "metadata": {
        "id": "xr7ck-Ekmrzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #上傳資料\n",
        "# Get the filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]\n",
        "data2 = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "DqoQ1RBrmuc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data2.dtypes)\n",
        "data2['n_lastname'] = data2['n_lastname'].fillna(data2['jeiba_lastname'])\n",
        "data2['n_lastname'] = data2['n_lastname'].fillna(data2['lastname'])"
      ],
      "metadata": {
        "id": "E7g1fPb6EDHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data2['n_lastname'].isnull().sum())"
      ],
      "metadata": {
        "id": "oQWv5XBZMWFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(new_data, model):\n",
        "    # 取得預測結果，注意這裡假設模型接受 ln_lastname 欄位作為輸入\n",
        "    # 將文本數據轉換為數值特徵\n",
        "    X_new = vectorizer.transform(new_data['n_lastname'])\n",
        "\n",
        "    predicted_ids = model.predict(X_new)\n",
        "\n",
        "    # 使用相同的 label_encoder 將編碼數字轉回原始標籤\n",
        "    predicted_labels = label_encoder.inverse_transform(predicted_ids)\n",
        "\n",
        "    # 嘗試獲取預測機率（部分模型可能不支援）\n",
        "    try:\n",
        "        probs = model.predict_proba(X_new)  # 取得所有類別的機率分布\n",
        "        max_probs = probs.max(axis=1)  # 取得最高的機率值\n",
        "    except AttributeError:\n",
        "        max_probs = [None] * len(predicted_ids)  # 若模型不支援，則填 None\n",
        "\n",
        "    # 將預測結果加入 DataFrame 中\n",
        "    new_data['predicted_label'] = predicted_labels\n",
        "\n",
        "    # 根據預測結果與原始 mainid 進行比較，產生 code 欄位 (1 表示預測正確，0 表示錯誤)\n",
        "    new_data['code'] = (new_data['predicted_label'] == new_data['mainid']).astype(int)\n",
        "    new_data['prediction_confidence'] = max_probs  # 新增預測機率欄位\n",
        "    # 計算準確率\n",
        "    accuracy = new_data['code'].mean()\n",
        "    return new_data, accuracy"
      ],
      "metadata": {
        "id": "2asFTPRoZJsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###將各模型結果儲存"
      ],
      "metadata": {
        "id": "0-NXv2uXu0z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#單一模型測試\n",
        "data3, accu = predict_and_evaluate(data2, svm_model_smote)\n",
        "print(f\"準確率: {accu:.4f}\")"
      ],
      "metadata": {
        "id": "5hHi4qSf0Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"rdf\": rdf_model,\n",
        "    \"rdf_smote\": rdf_model_smote,\n",
        "    \"nb\": nb_model,\n",
        "    \"nb_smote\": nb_model_smote,\n",
        "    \"svm\": svm_model,\n",
        "    \"svm_smote\": svm_model_smote,\n",
        "    \"lgbm\": lgbm_model,\n",
        "    \"lgbm_smote\": lgbm_model_smote\n",
        "}\n",
        "\n",
        "# 用字典存儲不同模型的預測結果\n",
        "results_df = {}  # 存放 DataFrame\n",
        "accuracies = {}  # 存放準確率\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    results_df[model_name], accuracies[model_name] = predict_and_evaluate(data2.copy(), model)\n",
        "    print(f\"{model_name}: Accuracy = {accuracies[model_name]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "njVi19EFl400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#將模型結果分別儲存\n",
        "rdf_df = results_df[\"rdf\"]\n",
        "rdf_smote_df = results_df[\"rdf_smote\"]\n",
        "nb_df = results_df[\"nb\"]\n",
        "nb_smote_df = results_df[\"nb_smote\"]\n",
        "svm_df = results_df[\"svm\"]\n",
        "svm_smote_df = results_df[\"svm_smote\"]\n",
        "lgbm_df = results_df[\"lgbm\"]\n",
        "lgbm_smote_df = results_df[\"lgbm_smote\"]"
      ],
      "metadata": {
        "id": "Yw3J_qiNpmKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, df in results_df.items():\n",
        "    filename = f\"{model_name}_predictions.csv\"  # 生成檔名\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "    files.download(filename)  # 下載檔案（適用於 Google Colab）\n",
        "    print(f\"✅ {filename} 已儲存！\")"
      ],
      "metadata": {
        "id": "W78ckWleruTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###檢查模型結果\n"
      ],
      "metadata": {
        "id": "thEBTcMsu8ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 選擇要合併的 DataFrame\n",
        "dfs_to_merge = [rdf_smote_df, nb_smote_df, svm_smote_df, lgbm_smote_df]\n",
        "\n",
        "# 先為每個 DataFrame 內的 predicted_label 和 code 加上專屬後綴，避免衝突\n",
        "suffixes = [\"_rdf\", \"_nb\", \"_svm\", \"_lgbm\"]\n",
        "for i, df in enumerate(dfs_to_merge):\n",
        "    df.rename(columns={\"predicted_label\": f\"predicted_label{suffixes[i]}\",\n",
        "              \"code\": f\"code{suffixes[i]}\",\n",
        "              \"prediction_confidence\" : f\"prediction_confidence{suffixes[i]}\"}, inplace=True)\n",
        "\n",
        "# 以第一個 DataFrame 作為基準，依次合併\n",
        "merged_smote_df = dfs_to_merge[0]\n",
        "for df in dfs_to_merge[1:]:\n",
        "    merged_smote_df = merged_smote_df.merge(\n",
        "        df,\n",
        "        on=[\"mainid\", \"lastname\", \"result\", \"n_lastname\", \"jeiba_lastname\",\"n_lastname_2\", \"n\", \"N\", \"k\"],\n",
        "        how=\"outer\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "EDwwVt9ku7LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_smote_df"
      ],
      "metadata": {
        "id": "efV6RYdziphh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##集成分類"
      ],
      "metadata": {
        "id": "Zn9J4edi6_lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 建立一個 DataFrame 存放所有模型的預測結果\n",
        "ensemble_results = pd.DataFrame()\n",
        "\n",
        "# 先加入真實標籤\n",
        "ensemble_results[\"true_label\"] = data2[\"mainid\"]\n",
        "ensemble_results[\"n_lastname\"] = data2[\"n_lastname\"]  # 加入 n_lastname\n",
        "# 加入各模型的預測標籤\n",
        "ensemble_results[\"rdf_pred\"] = rdf_smote_df['predicted_label_rdf']\n",
        "ensemble_results[\"nb_pred\"] = nb_smote_df['predicted_label_nb']\n",
        "ensemble_results[\"svm_pred\"] = svm_smote_df['predicted_label_svm']\n",
        "ensemble_results[\"lgbm_pred\"] = lgbm_smote_df['predicted_label_lgbm']\n",
        "\n",
        "# 加入個模型的預測機率\n",
        "ensemble_results[\"rdf_confidence\"] = rdf_smote_df['prediction_confidence_rdf']\n",
        "ensemble_results[\"nb_confidence\"] = nb_smote_df['prediction_confidence_nb']\n",
        "ensemble_results[\"svm_confidence\"] = svm_smote_df['prediction_confidence_svm']\n",
        "ensemble_results[\"lgbm_confidence\"] = lgbm_smote_df['prediction_confidence_lgbm']\n",
        "\n",
        "# 檢查結果\n",
        "#print(ensemble_results.head())"
      ],
      "metadata": {
        "id": "4BLk-r137L_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 設定權重（NB 最高，LGBM 次高，RF 最低）\n",
        "model_weights = {\n",
        "    \"nb_pred\": 0.5,   # NB 權重最高\n",
        "    \"lgbm_pred\": 0.2, # LGBM 權重低\n",
        "    \"rdf_pred\": 0.3    # 隨機森林權重中\n",
        "}\n",
        "\n",
        "def weighted_voting(row):\n",
        "    \"\"\"當 SVM 預測錯誤時，使用加權投票\"\"\"\n",
        "    svm_pred = row[\"svm_pred\"]\n",
        "    true_label = row[\"true_label\"]\n",
        "\n",
        "    #信心低於0.9\n",
        "    threshold = 0.90  # SVM 信心機率閾值\n",
        "    svm_pred = row[\"svm_pred\"]\n",
        "    svm_confidence = row[\"svm_confidence\"]  # SVM 預測的信心機率\n",
        "\n",
        "    # 若 SVM 信心高於 90%，則直接使用 SVM 預測結果\n",
        "    if pd.notna(svm_confidence) and svm_confidence >= threshold:\n",
        "        return svm_pred\n",
        "\n",
        "\n",
        "    # 使用 NB、LGBM、RF 進行加權投票\n",
        "    vote_scores = defaultdict(int)\n",
        "\n",
        "    for model, weight in model_weights.items():\n",
        "        predicted_class = row[model]  # 取得該模型的預測結果\n",
        "        vote_scores[predicted_class] += predicted_class * weight  # 累加該類別的權重\n",
        "\n",
        "    # 取得權重最高的類別\n",
        "    final_prediction = max(vote_scores, key=vote_scores.get)\n",
        "    return final_prediction\n",
        "\n",
        "# 套用加權投票修正\n",
        "ensemble_results[\"final_pred\"] = ensemble_results.apply(weighted_voting, axis=1)\n",
        "\n",
        "# 計算最終準確率\n",
        "final_accuracy = (ensemble_results[\"final_pred\"] == ensemble_results[\"true_label\"]).mean()\n",
        "print(f\"🎯 最終加權投票修正後的準確率：{final_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Rt8MksfO7B1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#輸出結果\n",
        "output_filename = \"預測結果比對.csv\"\n",
        "ensemble_results.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "QCpl1jR4oK5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###儲存模型"
      ],
      "metadata": {
        "id": "bJlzzE_vtvXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"rdf\": rdf_model,\n",
        "    \"rdf_smote\": rdf_model_smote,\n",
        "    \"nb\": nb_model,\n",
        "    \"nb_smote\": nb_model_smote,\n",
        "    \"svm\": svm_model,\n",
        "    \"svm_smote\": svm_model_smote,\n",
        "    \"lgbm\": lgbm_model,\n",
        "    \"lgbm_smote\": lgbm_model_smote\n",
        "}"
      ],
      "metadata": {
        "id": "SMho_Icj6u2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "# 儲存所有模型\n",
        "joblib.dump(svm_model_smote, 'svm.joblib')\n",
        "joblib.dump(nb_model_smote, 'naive_bayes.joblib')\n",
        "joblib.dump(rdf_model_smote, 'random_forest.joblib')\n",
        "joblib.dump(lgbm_model_smote, 'lightgbm.joblib')\n",
        "\n",
        "# 儲存向量化器 & 標籤編碼器\n",
        "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
        "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
        "\n",
        "\n",
        "# 下載所有模型\n",
        "for file_name in [\"svm.joblib\", \"naive_bayes.joblib\", \"random_forest.joblib\", \"lightgbm.joblib\",\n",
        "                  \"vectorizer.joblib\", \"label_encoder.joblib\"]:\n",
        "    files.download(file_name)\n",
        "\n",
        "print(\"✅ 所有模型已儲存並下載！\")\n"
      ],
      "metadata": {
        "id": "gq15jguWlIuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"random_forest.joblib\")"
      ],
      "metadata": {
        "id": "Mb3y5YmJCdxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 上傳模型進行使用"
      ],
      "metadata": {
        "id": "lwj-6zCivJqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 儲存向量化器 & 標籤編碼器\n",
        "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
        "joblib.dump(label_encoder, 'label_encoder.joblib')"
      ],
      "metadata": {
        "id": "aQ9Mtw1q3lZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# 載入模型\n",
        "model = joblib.load('naive_bayes.joblib')\n",
        "\n",
        "# 載入 TF-IDF vectorizer\n",
        "vectorizer = joblib.load('vectorizer.joblib')\n",
        "\n",
        "# 載入 LabelEncoder\n",
        "label_encoder = joblib.load('label_encoder.joblib')"
      ],
      "metadata": {
        "id": "O4AiCTBzvfnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#單一模型測試\n",
        "data3, accu = predict_and_evaluate(data2, model)\n",
        "print(f\"準確率: {accu:.4f}\")"
      ],
      "metadata": {
        "id": "jw9nyqiY3sjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vectorizer.get_feature_names_out()))  # 這應該是 3837\n"
      ],
      "metadata": {
        "id": "cIruxvYw4IKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

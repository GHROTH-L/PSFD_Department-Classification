#%%
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
import pandas as pd
import numpy as np
import joblib
from collections import Counter
import os 
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "1"

#è¼‰å…¥æ¨¡å‹
def load_models(Models_dir):
    
    model_dir=Models_dir  #ä¿®æ”¹æˆMoldesçš„æ‰€åœ¨åœ°

    # ç¢ºä¿è·¯å¾‘å­˜åœ¨
    if not os.path.exists(model_dir):
        raise FileNotFoundError(f" æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {model_dir}")

    # è¼‰å…¥ Vectorizer å’Œ LabelEncoder
    vectorizer_path = os.path.join(model_dir, "vectorizer.joblib")
    label_encoder_path = os.path.join(model_dir, "label_encoder.joblib")

    if not os.path.exists(vectorizer_path) or not os.path.exists(label_encoder_path):
        raise FileNotFoundError(" ç¼ºå°‘ vectorizer æˆ– label_encoderï¼Œè«‹æª¢æŸ¥æ¨¡å‹ç›®éŒ„ï¼")

    vectorizer = joblib.load(vectorizer_path)
    label_encoder = joblib.load(label_encoder_path)

    # å»ºç«‹æ¨¡å‹å­—å…¸
    model_files = {
        "rdf_smote": "random_forest.joblib",
        "nb_smote": "naive_bayes.joblib",
        "svm_smote": "svm.joblib",
        "lgbm_smote": "lightgbm.joblib",
    }

    models = {}
    for model_name, file_name in model_files.items():
        model_path = os.path.join(model_dir, file_name)
        if os.path.exists(model_path):
            models[model_name] = joblib.load(model_path)
        else:
            raise FileNotFoundError(f"ç¼ºå°‘æ¨¡å‹: {file_name}")

    print("æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    return vectorizer, label_encoder, models



# é æ¸¬èˆ‡è©•ä¼°ï¼ˆåŠ å…¥é æ¸¬æ©Ÿç‡ï¼‰
def predict_and_evaluate(new_data, model, vectorizer, label_encoder):
    # ç¢ºä¿è¼¸å…¥æ•¸æ“šåŒ…å«å¿…è¦çš„æ¬„ä½
    required_columns = {"n_lastname"}  #ä¸éœ€è¦æœ‰main_id
    missing_columns = required_columns - set(new_data.columns)
    if missing_columns:
        raise KeyError(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{', '.join(missing_columns)}")

    # å°‡æ–‡æœ¬æ•¸æ“šè½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µ
    X_new = vectorizer.transform(new_data['n_lastname'])

    # å–å¾—é æ¸¬çµæœ
    predicted_ids = model.predict(X_new)
    predicted_labels = label_encoder.inverse_transform(predicted_ids)

    # å˜—è©¦ç²å–é æ¸¬æ©Ÿç‡ï¼ˆéƒ¨åˆ†æ¨¡å‹å¯èƒ½ä¸æ”¯æ´ï¼‰
    try:
        probs = model.predict_proba(X_new)  # å–å¾—æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡åˆ†å¸ƒ
        max_probs = probs.max(axis=1)  # å–å¾—æœ€é«˜çš„æ©Ÿç‡å€¼
    except AttributeError:
        max_probs = [None] * len(predicted_ids)  # è‹¥æ¨¡å‹ä¸æ”¯æ´ï¼Œå‰‡å¡« None

    # å»ºç«‹æ–° DataFrameï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    new_data = new_data.copy()
    new_data['predicted_label'] = predicted_labels
    new_data['prediction_confidence'] = max_probs  # æ–°å¢é æ¸¬æ©Ÿç‡æ¬„ä½


    return new_data

#é æ¸¬æ‰€æœ‰æ¨¡å‹ï¼Œä¸¦ä¸”é€²è¡Œmerge
def merge_model_predictions(data2, models, predict_and_evaluate, vectorizer, label_encoder):

    # é æ¸¬æ‰€æœ‰æ¨¡å‹
    results_df = {}
    for model_name, model in models.items():
        results_df[model_name] = predict_and_evaluate(data2.copy(), model, vectorizer, label_encoder)

    # å®šç¾©æ¬„ä½å¾Œç¶´ï¼Œé¿å…æ¬„ä½åç¨±è¡çª
    suffixes = {
        "rdf_smote": "_rdf",
        "nb_smote": "_nb",
        "svm_smote": "_svm",
        "lgbm_smote": "_lgbm"
    }

    # é‡æ–°å‘½åå„æ¨¡å‹çš„é æ¸¬æ¬„ä½
    for model_name, df in results_df.items():
        df.rename(columns={
            "predicted_label": f"predicted_label{suffixes[model_name]}",
            "code": f"code{suffixes[model_name]}",
            "prediction_confidence": f"prediction_confidence{suffixes[model_name]}"
        }, inplace=True)

    # 4ï¸âƒ£ ä»¥ rdf_smote ä½œç‚ºåŸºæº–ï¼Œåˆä½µæ‰€æœ‰æ¨¡å‹é æ¸¬çµæœ
    merged_smote_df = results_df["rdf_smote"]
    for model_name in ["nb_smote", "svm_smote", "lgbm_smote"]:
        merged_smote_df = merged_smote_df.merge(
            results_df[model_name],
            on=["lastname", "result", "n_lastname", "jeiba_lastname"],
            how="outer"
        )
    print("æ‰€æœ‰æ¨¡å‹é æ¸¬çµæœå·²ç¶“merge")
    return merged_smote_df, results_df , suffixes

#æŠ•ç¥¨è¡¨æ±º
def weighted_voting(row):
    """ç•¶ SVM ä¿¡å¿ƒæ©Ÿç‡ < 90% æ™‚ï¼Œä½¿ç”¨åŠ æ¬ŠæŠ•ç¥¨"""
    model_weights = {
        "nb_smote_pred": 3,   # NB æ¬Šé‡æœ€é«˜
        "lgbm_smote_pred": 1, # LGBM æ¬Šé‡æœ€ä½
        "rdf_smote_pred": 2    # éš¨æ©Ÿæ£®æ—æ¬Šé‡ ä¸­é–“
    }

    threshold = 0.90  # SVM ä¿¡å¿ƒæ©Ÿç‡é–¾å€¼
    svm_pred = row["svm_smote_pred"]
    svm_confidence = row["prediction_confidence_svm_smote"]  # SVM é æ¸¬çš„ä¿¡å¿ƒæ©Ÿç‡

    # è‹¥ SVM ä¿¡å¿ƒé«˜æ–¼ 90%ï¼Œå‰‡ç›´æ¥ä½¿ç”¨ SVM é æ¸¬çµæœ
    if pd.notna(svm_confidence) and svm_confidence >= threshold:
        return svm_pred

    # ä½¿ç”¨åŠ æ¬ŠæŠ•ç¥¨
    vote_scores = Counter()
    for model, weight in model_weights.items():
        if pd.notna(row[model]):  # ç¢ºä¿ä¸è™•ç† NaN
            vote_scores[row[model]] += weight

    # å–å¾—æ¬Šé‡æœ€é«˜çš„é¡åˆ¥
    if vote_scores:
        return max(vote_scores, key=vote_scores.get)
    else:
        return svm_pred  # è‹¥å…¶ä»–æ¨¡å‹ç„¡æ³•æä¾›çµæœï¼Œå›å‚³ SVM é æ¸¬å€¼

#é›†æˆ
def ensemble_voting(data2, models, results_df, suffixes, weighted_voting):

    # å»ºç«‹ DataFrame å­˜æ”¾æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ
    ensemble_results = pd.DataFrame()

    # åŠ å…¥å„æ¨¡å‹çš„é æ¸¬æ¨™ç±¤èˆ‡æ©Ÿç‡
    for model_name in models.keys():
        pred_col = f"predicted_label{suffixes.get(model_name, '')}"
        conf_col = f"prediction_confidence{suffixes.get(model_name, '')}"
        
        # æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨ï¼Œé¿å… KeyError
        if pred_col in results_df[model_name].columns:
            ensemble_results[f"{model_name}_pred"] = results_df[model_name][pred_col]
        else:
            print(f"{model_name} ç¼ºå°‘ {pred_col}ï¼Œè·³éï¼")

        if conf_col in results_df[model_name].columns:
            ensemble_results[f"prediction_confidence_{model_name}"] = results_df[model_name][conf_col]
        else:
            print(f"{model_name} ç¼ºå°‘ {conf_col}ï¼Œè·³éï¼")

    # å¥—ç”¨åŠ æ¬ŠæŠ•ç¥¨
    ensemble_results["final_pred"] = ensemble_results.apply(weighted_voting, axis=1)

    return ensemble_results  

def predict_department_text(text, models, vectorizer, label_encoder, weighted_voting):

    if not isinstance(text, str) or len(text.strip()) == 0:
        raise ValueError("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å­¸ç³»åç¨±ï¼")

    # å»ºç«‹ DataFrame æ ¼å¼ï¼Œä»¥ç¬¦åˆ `merge_model_predictions()` çš„éœ€æ±‚
    data = pd.DataFrame({"n_lastname": [text]})
    data["lastname"] = data["n_lastname"]
    data["result"] = data["n_lastname"]
    data["jeiba_lastname"] = data["n_lastname"]

    # é€²è¡Œæ‰€æœ‰æ¨¡å‹é æ¸¬ä¸¦åˆä½µçµæœ
    merged_smote_df, results_df , suffixes = merge_model_predictions(data, models, predict_and_evaluate, vectorizer, label_encoder)

    # å¥—ç”¨åŠ æ¬ŠæŠ•ç¥¨ï¼Œä¸¦å–å¾—æœ€çµ‚å­¸ç³»é æ¸¬
    final_prediction = ensemble_voting(merged_smote_df, models, results_df, suffixes, weighted_voting)

    return final_prediction
#%%

if __name__ == "__main__":
    #%%
    # è¼‰å…¥Models
    MODEL_DIR = "E:/PSFD_Department-Classification/Models"
    vectorizer, label_encoder, models = load_models(MODEL_DIR)
    #%%
    # ä¸Šè¼‰æƒ³è¦é æ¸¬çš„æ•¸æ“š
    data2 = pd.read_csv("C:/Users/user/Downloads/data.csv", encoding="utf-8")

    # åŸ·è¡Œé æ¸¬ä¸¦åˆä½µçµæœ
    merged_smote_df, results_df , suffixes = merge_model_predictions(data2, models, predict_and_evaluate, vectorizer, label_encoder)
    #%%
    #é€²è¡Œçµæœè¼¸å‡º
    ensemble_results = ensemble_voting(data2, models, results_df, suffixes, weighted_voting)

    # å„²å­˜çµæœ
    output_file="final_predictions.csv"
    ensemble_results.to_csv(output_file, index=False)
    print(f"âœ…  å·²å„²å­˜è‡³ {output_file}")
    #%%
    # æ¸¬è©¦å–®ç´”è¼¸å…¥æ–‡å­—
    text_input = "æ³•å¾‹"
    final_pred = predict_department_text(text_input, models, vectorizer, label_encoder, weighted_voting)
    print(f"ğŸ¯ æœ€çµ‚é æ¸¬å­¸ç³»ï¼š{final_pred['final_pred']}")
#%%

